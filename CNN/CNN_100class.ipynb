{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-07 16:46:17--  https://raw.githubusercontent.com/zaidalyafeai/zaidalyafeai.github.io/master/sketcher/mini_classes.txt\n",
      "Resolving raw.githubusercontent.com... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 760 [text/plain]\n",
      "Saving to: `mini_classes.txt'\n",
      "\n",
      "100%[======================================>] 760         --.-K/s   in 0s      \n",
      "\n",
      "2018-12-07 16:46:17 (171 MB/s) - `mini_classes.txt' saved [760/760]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/zaidalyafeai/zaidalyafeai.github.io/master/sketcher/mini_classes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"mini_classes.txt\",\"r\")\n",
    "# And for reading use\n",
    "classes = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [c.replace('\\n','').replace(' ','_') for c in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "def download():\n",
    "    base = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
    "    for c in classes:\n",
    "        cls_url = c.replace('_', '%20')\n",
    "        path = base+cls_url+'.npy'\n",
    "        print(path)\n",
    "        urllib.request.urlretrieve(path, 'data_all/'+c+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/drums.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sun.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/laptop.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/anvil.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/baseball%20bat.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ladder.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/eyeglasses.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/grapes.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/book.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dumbbell.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/traffic%20light.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/wristwatch.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/wheel.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/shovel.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bread.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/table.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tennis%20racquet.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cloud.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/chair.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/headphones.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/face.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/eye.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/snake.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/lollipop.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/power%20outlet.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pants.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/mushroom.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/star.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sword.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/clock.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hot%20dog.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/syringe.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/stop%20sign.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/mountain.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/smiley%20face.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/apple.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bed.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/shorts.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broom.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/diving%20board.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/flower.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/spider.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cell%20phone.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/car.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/camera.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tree.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/square.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/moon.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/radio.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hat.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pizza.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/axe.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/door.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tent.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/umbrella.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/line.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cup.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fan.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/triangle.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/basketball.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pillow.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/scissors.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/t-shirt.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tooth.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/alarm%20clock.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/paper%20clip.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/spoon.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/microphone.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/candle.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pencil.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/envelope.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/saw.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/frying%20pan.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/screwdriver.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/helmet.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bridge.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/light%20bulb.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ceiling%20fan.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/key.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/donut.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bird.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/circle.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/beard.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/coffee%20cup.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/butterfly.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bench.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rifle.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cat.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sock.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ice%20cream.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/moustache.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/suitcase.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hammer.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rainbow.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/knife.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cookie.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/baseball.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/lightning.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bicycle.npy\n"
     ]
    }
   ],
   "source": [
    "download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(vfold_ratio=0.2, max_items_per_class= 5000 ):\n",
    "    all_files = glob.glob(os.path.join(\"./data_all/\", '*.npy'))\n",
    "\n",
    "    #initialize variables \n",
    "    x = np.empty([0, 784])\n",
    "    y = np.empty([0])\n",
    "    class_names = []\n",
    "\n",
    "    #load a subset of the data to memory \n",
    "    for idx, file in enumerate(all_files):\n",
    "        data = np.load(file)\n",
    "        data = data[0: max_items_per_class, :]\n",
    "        labels = np.full(data.shape[0], idx)\n",
    "\n",
    "        x = np.concatenate((x, data), axis=0)\n",
    "        y = np.append(y, labels)\n",
    "\n",
    "        class_name, ext = os.path.splitext(os.path.basename(file))\n",
    "        class_names.append(class_name)\n",
    "\n",
    "    data = None\n",
    "    labels = None\n",
    "\n",
    "    #separate into training and testing \n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    x = x[permutation, :]\n",
    "    y = y[permutation]\n",
    "\n",
    "    vfold_size = int(x.shape[0]/100*(vfold_ratio*100))\n",
    "\n",
    "    x_test = x[0:vfold_size, :]\n",
    "    y_test = y[0:vfold_size]\n",
    "\n",
    "    x_train = x[vfold_size:x.shape[0], :]\n",
    "    y_train = y[vfold_size:y.shape[0]]\n",
    "    \n",
    "#     print(x_train)\n",
    "    return x_train, y_train, x_test, y_test, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, class_names = load_data()\n",
    "num_classes = len(class_names)\n",
    "image_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syringe\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b4a515bb3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "%matplotlib inline  \n",
    "idx = randint(0, len(x_train))\n",
    "plt.imshow(x_train[idx].reshape(28,28)) \n",
    "print(class_names[int(y_train[idx].item())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and normalize\n",
    "x_train = x_train.reshape(x_train.shape[0], image_size, image_size, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], image_size, image_size, 1).astype('float32')\n",
    "\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "# Convert class vectors to class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 3, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               16500     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               50100     \n",
      "=================================================================\n",
      "Total params: 144,328\n",
      "Trainable params: 144,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Convolution2D(filters=32, kernel_size=[5, 5], padding='same', \n",
    "                        input_shape=x_train.shape[1:], activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(layers.Convolution2D(filters=32, kernel_size=[5, 5], padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(layers.Convolution2D(filters=32, kernel_size = [5, 5], padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(layers.Convolution2D(filters=32, kernel_size=[5, 5], padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(units=500, activation='relu'))\n",
    "model.add(layers.Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "adam = tf.train.AdamOptimizer()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['top_k_categorical_accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400000 samples, validate on 100000 samples\n",
      "Epoch 1/34\n",
      "18s - loss: 1.1245 - top_k_categorical_accuracy: 0.9078 - val_loss: 1.0217 - val_top_k_categorical_accuracy: 0.9187\n",
      "Epoch 2/34\n",
      "17s - loss: 0.9728 - top_k_categorical_accuracy: 0.9231 - val_loss: 0.9614 - val_top_k_categorical_accuracy: 0.9244\n",
      "Epoch 3/34\n",
      "17s - loss: 0.8943 - top_k_categorical_accuracy: 0.9309 - val_loss: 0.9413 - val_top_k_categorical_accuracy: 0.9274\n",
      "Epoch 4/34\n",
      "17s - loss: 0.8443 - top_k_categorical_accuracy: 0.9353 - val_loss: 0.8806 - val_top_k_categorical_accuracy: 0.9326\n",
      "Epoch 5/34\n",
      "17s - loss: 0.8098 - top_k_categorical_accuracy: 0.9389 - val_loss: 0.8587 - val_top_k_categorical_accuracy: 0.9336\n",
      "Epoch 6/34\n",
      "17s - loss: 0.7836 - top_k_categorical_accuracy: 0.9411 - val_loss: 0.8653 - val_top_k_categorical_accuracy: 0.9329\n",
      "Epoch 7/34\n",
      "17s - loss: 0.7611 - top_k_categorical_accuracy: 0.9432 - val_loss: 0.8325 - val_top_k_categorical_accuracy: 0.9360\n",
      "Epoch 8/34\n",
      "17s - loss: 0.7433 - top_k_categorical_accuracy: 0.9446 - val_loss: 0.8729 - val_top_k_categorical_accuracy: 0.9331\n",
      "Epoch 9/34\n",
      "17s - loss: 0.7295 - top_k_categorical_accuracy: 0.9462 - val_loss: 0.8241 - val_top_k_categorical_accuracy: 0.9369\n",
      "Epoch 10/34\n",
      "17s - loss: 0.7164 - top_k_categorical_accuracy: 0.9476 - val_loss: 0.8369 - val_top_k_categorical_accuracy: 0.9366\n",
      "Epoch 11/34\n",
      "17s - loss: 0.7048 - top_k_categorical_accuracy: 0.9482 - val_loss: 0.8188 - val_top_k_categorical_accuracy: 0.9372\n",
      "Epoch 12/34\n",
      "17s - loss: 0.6936 - top_k_categorical_accuracy: 0.9494 - val_loss: 0.8391 - val_top_k_categorical_accuracy: 0.9344\n",
      "Epoch 13/34\n",
      "17s - loss: 0.6856 - top_k_categorical_accuracy: 0.9500 - val_loss: 0.8461 - val_top_k_categorical_accuracy: 0.9355\n",
      "Epoch 14/34\n",
      "17s - loss: 0.6771 - top_k_categorical_accuracy: 0.9509 - val_loss: 0.8485 - val_top_k_categorical_accuracy: 0.9354\n",
      "Epoch 15/34\n",
      "17s - loss: 0.6717 - top_k_categorical_accuracy: 0.9514 - val_loss: 0.8417 - val_top_k_categorical_accuracy: 0.9339\n",
      "Epoch 16/34\n",
      "17s - loss: 0.6632 - top_k_categorical_accuracy: 0.9525 - val_loss: 0.8504 - val_top_k_categorical_accuracy: 0.9335\n",
      "Epoch 17/34\n",
      "17s - loss: 0.6562 - top_k_categorical_accuracy: 0.9529 - val_loss: 0.8521 - val_top_k_categorical_accuracy: 0.9351\n",
      "Epoch 18/34\n",
      "17s - loss: 0.6521 - top_k_categorical_accuracy: 0.9531 - val_loss: 0.8642 - val_top_k_categorical_accuracy: 0.9349\n",
      "Epoch 19/34\n",
      "17s - loss: 0.6473 - top_k_categorical_accuracy: 0.9534 - val_loss: 0.8510 - val_top_k_categorical_accuracy: 0.9352\n",
      "Epoch 20/34\n",
      "17s - loss: 0.6406 - top_k_categorical_accuracy: 0.9545 - val_loss: 0.8506 - val_top_k_categorical_accuracy: 0.9340\n",
      "Epoch 21/34\n",
      "17s - loss: 0.6366 - top_k_categorical_accuracy: 0.9546 - val_loss: 0.8698 - val_top_k_categorical_accuracy: 0.9350\n",
      "Epoch 22/34\n",
      "17s - loss: 0.6325 - top_k_categorical_accuracy: 0.9553 - val_loss: 0.8587 - val_top_k_categorical_accuracy: 0.9350\n",
      "Epoch 23/34\n",
      "17s - loss: 0.6290 - top_k_categorical_accuracy: 0.9558 - val_loss: 0.8657 - val_top_k_categorical_accuracy: 0.9349\n",
      "Epoch 24/34\n",
      "17s - loss: 0.6253 - top_k_categorical_accuracy: 0.9559 - val_loss: 0.8642 - val_top_k_categorical_accuracy: 0.9333\n",
      "Epoch 25/34\n",
      "17s - loss: 0.6215 - top_k_categorical_accuracy: 0.9566 - val_loss: 0.8895 - val_top_k_categorical_accuracy: 0.9335\n",
      "Epoch 26/34\n",
      "17s - loss: 0.6199 - top_k_categorical_accuracy: 0.9565 - val_loss: 0.8769 - val_top_k_categorical_accuracy: 0.9322\n",
      "Epoch 27/34\n",
      "17s - loss: 0.6163 - top_k_categorical_accuracy: 0.9570 - val_loss: 0.8687 - val_top_k_categorical_accuracy: 0.9340\n",
      "Epoch 28/34\n",
      "17s - loss: 0.6117 - top_k_categorical_accuracy: 0.9573 - val_loss: 0.8834 - val_top_k_categorical_accuracy: 0.9323\n",
      "Epoch 29/34\n",
      "17s - loss: 0.6093 - top_k_categorical_accuracy: 0.9579 - val_loss: 0.8850 - val_top_k_categorical_accuracy: 0.9328\n",
      "Epoch 30/34\n",
      "17s - loss: 0.6077 - top_k_categorical_accuracy: 0.9577 - val_loss: 0.8935 - val_top_k_categorical_accuracy: 0.9329\n",
      "Epoch 31/34\n",
      "17s - loss: 0.6059 - top_k_categorical_accuracy: 0.9581 - val_loss: 0.9037 - val_top_k_categorical_accuracy: 0.9306\n",
      "Epoch 32/34\n",
      "17s - loss: 0.6041 - top_k_categorical_accuracy: 0.9583 - val_loss: 0.8889 - val_top_k_categorical_accuracy: 0.9322\n",
      "Epoch 33/34\n",
      "17s - loss: 0.6021 - top_k_categorical_accuracy: 0.9586 - val_loss: 0.9172 - val_top_k_categorical_accuracy: 0.9295\n",
      "Epoch 34/34\n",
      "17s - loss: 0.5989 - top_k_categorical_accuracy: 0.9590 - val_loss: 0.8963 - val_top_k_categorical_accuracy: 0.9324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b50b4b9b6d8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=34,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.24%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
